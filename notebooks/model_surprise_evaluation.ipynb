{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to calculate precision@k and recall @k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import send_status_mail as ssm\n",
    "import joblib\n",
    "from surprise import Dataset, Reader\n",
    "\n",
    "df = joblib.load('../data/processed/preprocessed_data_movielens.pkl')\n",
    "df.drop(columns=['title','genres','relevance','tag'], inplace=True)\n",
    "# sort columns in required order\n",
    "df = df[['userId', 'movieId', 'rating']]\n",
    "# reset index, which was nonsense after import\n",
    "df = df.reset_index().drop(columns=['index'])\n",
    "\n",
    "# Load the data into Surprise format, columns have been sorted in required order (raw user id, raw item id, rating) beforehand\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(df, reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import KFold\n",
    "\n",
    "# instantiate kf with random_state for reproducibility\n",
    "kf = KFold(n_splits=5, random_state=42)\n",
    "\n",
    "# define measure for which GridSearch results will be retrieved\n",
    "measure = 'mae'\n",
    "\n",
    "# define evaluation parameters\n",
    "n_rec = [3,5,10,20] # number of recommendations => top k\n",
    "threshold = 3.5 # threshold for relevant recommendations (real rating >= threshold => relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve precision@k and recall@k for all models using k-fold prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### knn-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from surprise import KNNBasic\n",
    "\n",
    "# # import results from parameter tuning\n",
    "# gs_result = joblib.load('../models/surp_gridsearchcv_knnBasic.pkl')\n",
    "\n",
    "# # instantiate model with winner parameters from GridSearch according to desired measure, e.g. MAE\n",
    "# algo = KNNBasic(sim_options=gs_result.best_params[measure]['sim_options'], k= gs_result.best_params[measure]['k'], min_k=gs_result.best_params[measure]['min_k'])\n",
    "\n",
    "\n",
    "# # initiate dicts holding lists of average precision/recall for different k (n_rec) respectively\n",
    "# # empty lists will be filled in the next loop, iteration over splits\n",
    "# precisions_knnBasic_dict, recalls_knnBasic_dict = {}, {}\n",
    "# for k in n_rec:\n",
    "#     precisions_knnBasic_dict[k] = []\n",
    "#     recalls_knnBasic_dict[k] = []\n",
    "\n",
    "# # iterate over all splits, for each split: train model, predict, retrieve precision/recall for top k recommendations\n",
    "# for trainset, testset in kf.split(data):\n",
    "#     algo.fit(trainset)\n",
    "#     predictions = algo.test(testset)\n",
    "#     # iterate over all n_rec (number of recommendations): retrieve precision/recall for top k recommendations\n",
    "#     for k in n_rec:\n",
    "#         precisions, recalls = precision_recall_at_k(predictions, k, threshold)\n",
    "\n",
    "#         # Precision and recall can then be averaged over all users\n",
    "#         precisions_knnBasic_dict[k].append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "#         recalls_knnBasic_dict[k].append(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "\n",
    "# # send completion message via email (server, sender, recepient according to .env)\n",
    "# ssm.sendstatus(\"knnBasic precision/recall@k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from surprise import KNNWithMeans\n",
    "\n",
    "# # import results from parameter tuning\n",
    "# gs_result = joblib.load('../models/surp_gridsearchcv_knnMeans.pkl')\n",
    "\n",
    "# # instantiate model with winner parameters from GridSearch according to desired measure, e.g. MAE\n",
    "# algo = KNNWithMeans(sim_options=gs_result.best_params[measure]['sim_options'], k= gs_result.best_params[measure]['k'], min_k=gs_result.best_params[measure]['min_k'])\n",
    "\n",
    "\n",
    "# # initiate dicts holding lists of average precision/recall for different k (n_rec) respectively\n",
    "# # empty lists will be filled in the next loop, iteration over splits\n",
    "# precisions_knnMean_dict, recalls_knnMean_dict = {}, {}\n",
    "# for k in n_rec:\n",
    "#     precisions_knnMean_dict[k] = []\n",
    "#     recalls_knnMean_dict[k] = []\n",
    "\n",
    "# # iterate over all splits, for each split: train model, predict, retrieve precision/recall for top k recommendations\n",
    "# for trainset, testset in kf.split(data):\n",
    "#     algo.fit(trainset)\n",
    "#     predictions = algo.test(testset)\n",
    "#     # iterate over all n_rec (number of recommendations): retrieve precision/recall for top k recommendations\n",
    "#     for k in n_rec:\n",
    "#         precisions, recalls = precision_recall_at_k(predictions, k, threshold)\n",
    "\n",
    "#         # Precision and recall can then be averaged over all users\n",
    "#         precisions_knnMean_dict[k].append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "#         recalls_knnMean_dict[k].append(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "\n",
    "# # send completion message via email (server, sender, recepient according to .env)\n",
    "# ssm.sendstatus(\"knnMeans precision/recall@k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from surprise import KNNBaseline\n",
    "\n",
    "# # import results from parameter tuning\n",
    "# gs_result = joblib.load('../models/surp_gridsearchcv_knnBaseline.pkl')\n",
    "\n",
    "# # instantiate model with winner parameters from GridSearch according to desired measure, e.g. MAE\n",
    "# algo = KNNBaseline(sim_options=gs_result.best_params[measure]['sim_options'], k= gs_result.best_params[measure]['k'], min_k=gs_result.best_params[measure]['min_k'])\n",
    "\n",
    "\n",
    "# # initiate dicts holding lists of average precision/recall for different k (n_rec) respectively\n",
    "# # empty lists will be filled in the next loop, iteration over splits\n",
    "# precisions_knnBaseline_dict, recalls_knnBaseline_dict = {}, {}\n",
    "# for k in n_rec:\n",
    "#     precisions_knnBaseline_dict[k] = []\n",
    "#     recalls_knnBaseline_dict[k] = []\n",
    "\n",
    "# # iterate over all splits, for each split: train model, predict, retrieve precision/recall for top k recommendations\n",
    "# for trainset, testset in kf.split(data):\n",
    "#     algo.fit(trainset)\n",
    "#     predictions = algo.test(testset)\n",
    "#     # iterate over all n_rec (number of recommendations): retrieve precision/recall for top k recommendations\n",
    "#     for k in n_rec:\n",
    "#         precisions, recalls = precision_recall_at_k(predictions, k, threshold)\n",
    "\n",
    "#         # Precision and recall can then be averaged over all users\n",
    "#         precisions_knnBaseline_dict[k].append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "#         recalls_knnBaseline_dict[k].append(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "\n",
    "# # send completion message via email (server, sender, recepient according to .env)\n",
    "# ssm.sendstatus(\"knnBaseline precision/recall@k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from surprise import KNNWithZScore\n",
    "\n",
    "# # import results from parameter tuning\n",
    "# gs_result = joblib.load('../models/surp_gridsearchcv_knnZScore.pkl')\n",
    "\n",
    "# # instantiate model with winner parameters from GridSearch according to desired measure, e.g. MAE\n",
    "# algo = KNNWithZScore(sim_options=gs_result.best_params[measure]['sim_options'], k= gs_result.best_params[measure]['k'], min_k=gs_result.best_params[measure]['min_k'])\n",
    "\n",
    "\n",
    "# # initiate dicts holding lists of average precision/recall for different k (n_rec) respectively\n",
    "# # empty lists will be filled in the next loop, iteration over splits\n",
    "# precisions_knnZScore_dict, recalls_knnZScore_dict = {}, {}\n",
    "# for k in n_rec:\n",
    "#     precisions_knnZScore_dict[k] = []\n",
    "#     recalls_knnZScore_dict[k] = []\n",
    "\n",
    "# # iterate over all splits, for each split: train model, predict, retrieve precision/recall for top k recommendations\n",
    "# for trainset, testset in kf.split(data):\n",
    "#     algo.fit(trainset)\n",
    "#     predictions = algo.test(testset)\n",
    "#     # iterate over all n_rec (number of recommendations): retrieve precision/recall for top k recommendations\n",
    "#     for k in n_rec:\n",
    "#         precisions, recalls = precision_recall_at_k(predictions, k, threshold)\n",
    "\n",
    "#         # Precision and recall can then be averaged over all users\n",
    "#         precisions_knnZScore_dict[k].append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "#         recalls_knnZScore_dict[k].append(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "\n",
    "# # send completion message via email (server, sender, recepient according to .env)\n",
    "# ssm.sendstatus(\"knnZScore precision/recall@k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matrix factorization models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from surprise import SVD\n",
    "\n",
    "# # import results from parameter tuning\n",
    "# gs_result = joblib.load('../models/surp_gridsearchcv_SVD.pkl')\n",
    "\n",
    "# # instantiate model with winner parameters from GridSearch according to desired measure, e.g. MAE\n",
    "# algo = SVD(n_factors=gs_result.best_params[measure]['n_factors'],\n",
    "#            n_epochs=gs_result.best_params[measure]['n_epochs'],\n",
    "#            biased=gs_result.best_params[measure]['biased'],\n",
    "#            lr_all=gs_result.best_params[measure]['lr_all'],\n",
    "#            reg_all=gs_result.best_params[measure]['reg_all'],\n",
    "#            random_state=42)\n",
    "\n",
    "# # initiate dicts holding lists of average precision/recall for different k (n_rec) respectively\n",
    "# # empty lists will be filled in the next loop, iteration over splits\n",
    "# precisions_SVD_dict, recalls_SVD_dict = {}, {}\n",
    "# for k in n_rec:\n",
    "#     precisions_SVD_dict[k] = []\n",
    "#     recalls_SVD_dict[k] = []\n",
    "\n",
    "# # iterate over all splits, for each split: train model, predict, retrieve precision/recall for top k recommendations\n",
    "# for trainset, testset in kf.split(data):\n",
    "#     algo.fit(trainset)\n",
    "#     predictions = algo.test(testset)\n",
    "#     # iterate over all n_rec (number of recommendations): retrieve precision/recall for top k recommendations\n",
    "#     for k in n_rec:\n",
    "#         precisions, recalls = precision_recall_at_k(predictions, k, threshold)\n",
    "\n",
    "#         # Precision and recall can then be averaged over all users\n",
    "#         precisions_SVD_dict[k].append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "#         recalls_SVD_dict[k].append(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "\n",
    "# # send completion message via email (server, sender, recepient according to .env)\n",
    "# ssm.sendstatus(\"SVD precision/recall@k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from surprise import NMF\n",
    "\n",
    "# # import results from parameter tuning\n",
    "# gs_result = joblib.load('../models/surp_gridsearchcv_NMF.pkl')\n",
    "\n",
    "# # instantiate model with winner parameters from GridSearch according to desired measure, e.g. MAE\n",
    "# algo = NMF(n_factors=gs_result.best_params[measure]['n_factors'],\n",
    "#            n_epochs=gs_result.best_params[measure]['n_epochs'],\n",
    "#            biased=gs_result.best_params[measure]['biased'],\n",
    "#            reg_pu=gs_result.best_params[measure]['reg_pu'],\n",
    "#            reg_qi=gs_result.best_params[measure]['reg_qi'],\n",
    "#            random_state=42)\n",
    "\n",
    "# # initiate dicts holding lists of average precision/recall for different k (n_rec) respectively\n",
    "# # empty lists will be filled in the next loop, iteration over splits\n",
    "# precisions_NMF_dict, recalls_NMF_dict = {}, {}\n",
    "# for k in n_rec:\n",
    "#     precisions_NMF_dict[k] = []\n",
    "#     recalls_NMF_dict[k] = []\n",
    "\n",
    "# # iterate over all splits, for each split: train model, predict, retrieve precision/recall for top k recommendations\n",
    "# for trainset, testset in kf.split(data):\n",
    "#     algo.fit(trainset)\n",
    "#     predictions = algo.test(testset)\n",
    "#     # iterate over all n_rec (number of recommendations): retrieve precision/recall for top k recommendations\n",
    "#     for k in n_rec:\n",
    "#         precisions, recalls = precision_recall_at_k(predictions, k, threshold)\n",
    "\n",
    "#         # Precision and recall can then be averaged over all users\n",
    "#         precisions_NMF_dict[k].append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "#         recalls_NMF_dict[k].append(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "\n",
    "# # send completion message via email (server, sender, recepient according to .env)\n",
    "# ssm.sendstatus(\"NMF precision/recall@k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some algorithms randomly initialize their parameters (sometimes with numpy), and the cross-validation folds are also randomly generated. \n",
    "If you need to reproduce your experiments multiple times, you just have to set the seed of the RNG at the beginning of your program:\"\"\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "my_seed = 42\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import BaselineOnly\n",
    "\n",
    "# import results from parameter tuning\n",
    "gs_result = joblib.load('../models/surp_gridsearchcv_BaselineOnly.pkl')\n",
    "\n",
    "# instantiate model with winner parameters from GridSearch according to desired measure, e.g. MAE\n",
    "algo = BaselineOnly(bsl_options=gs_result.best_params[measure]['bsl_options'])\n",
    "\n",
    "# initiate dicts holding lists of average precision/recall for different k (n_rec) respectively\n",
    "# empty lists will be filled in the next loop, iteration over splits\n",
    "precisions_Baseline_dict, recalls_Baseline_dict = {}, {}\n",
    "for k in n_rec:\n",
    "    precisions_Baseline_dict[k] = []\n",
    "    recalls_Baseline_dict[k] = []\n",
    "\n",
    "# iterate over all splits, for each split: train model, predict, retrieve precision/recall for top k recommendations\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    # iterate over all n_rec (number of recommendations): retrieve precision/recall for top k recommendations\n",
    "    for k in n_rec:\n",
    "        precisions, recalls = precision_recall_at_k(predictions, k, threshold)\n",
    "\n",
    "        # Precision and recall can then be averaged over all users\n",
    "        precisions_Baseline_dict[k].append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "        recalls_Baseline_dict[k].append(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "\n",
    "# send completion message via email (server, sender, recepient according to .env)\n",
    "ssm.sendstatus(\"BaselineOnly precision/recall@k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import CoClustering\n",
    "\n",
    "# import results from parameter tuning\n",
    "gs_result = joblib.load('../models/surp_gridsearchcv_CoClustering.pkl')\n",
    "\n",
    "# instantiate model with winner parameters from GridSearch according to desired measure, e.g. MAE\n",
    "algo = CoClustering(n_cltr_u=gs_result.best_params[measure]['n_cltr_u'],\n",
    "                    n_cltr_i=gs_result.best_params[measure]['n_cltr_i'],\n",
    "                    n_epochs=gs_result.best_params[measure]['n_epochs'],\n",
    "                    random_state=42)\n",
    "\n",
    "# initiate dicts holding lists of average precision/recall for different k (n_rec) respectively\n",
    "# empty lists will be filled in the next loop, iteration over splits\n",
    "precisions_CC_dict, recalls_CC_dict = {}, {}\n",
    "for k in n_rec:\n",
    "    precisions_CC_dict[k] = []\n",
    "    recalls_CC_dict[k] = []\n",
    "\n",
    "# iterate over all splits, for each split: train model, predict, retrieve precision/recall for top k recommendations\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    # iterate over all n_rec (number of recommendations): retrieve precision/recall for top k recommendations\n",
    "    for k in n_rec:\n",
    "        precisions, recalls = precision_recall_at_k(predictions, k, threshold)\n",
    "\n",
    "        # Precision and recall can then be averaged over all users\n",
    "        precisions_CC_dict[k].append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "        recalls_CC_dict[k].append(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "\n",
    "# send completion message via email (server, sender, recepient according to .env)\n",
    "ssm.sendstatus(\"CC precision/recall@k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NormalPredictor\n",
    "\n",
    "# instantiate model\n",
    "# no GridSearch was performed, since algorithm does not take any arguments\n",
    "algo = NormalPredictor()\n",
    "\n",
    "# initiate dicts holding lists of average precision/recall for different k (n_rec) respectively\n",
    "# empty lists will be filled in the next loop, iteration over splits\n",
    "precisions_rand_dict, recalls_rand_dict = {}, {}\n",
    "for k in n_rec:\n",
    "    precisions_rand_dict[k] = []\n",
    "    recalls_rand_dict[k] = []\n",
    "\n",
    "# iterate over all splits, for each split: train model, predict, retrieve precision/recall for top k recommendations\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    # iterate over all n_rec (number of recommendations): retrieve precision/recall for top k recommendations\n",
    "    for k in n_rec:\n",
    "        precisions, recalls = precision_recall_at_k(predictions, k, threshold)\n",
    "\n",
    "        # Precision and recall can then be averaged over all users\n",
    "        precisions_rand_dict[k].append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "        recalls_rand_dict[k].append(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "\n",
    "# send completion message via email (server, sender, recepient according to .env)\n",
    "ssm.sendstatus(\"NormalPredictor precision/recall@k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SlopeOne\n",
    "\n",
    "# instantiate model\n",
    "# no GridSearch was performed, since algorithm does not take any arguments\n",
    "algo = SlopeOne()\n",
    "\n",
    "# initiate dicts holding lists of average precision/recall for different k (n_rec) respectively\n",
    "# empty lists will be filled in the next loop, iteration over splits\n",
    "precisions_SlopeOne_dict, recalls_SlopeOne_dict = {}, {}\n",
    "for k in n_rec:\n",
    "    precisions_SlopeOne_dict[k] = []\n",
    "    recalls_SlopeOne_dict[k] = []\n",
    "\n",
    "# iterate over all splits, for each split: train model, predict, retrieve precision/recall for top k recommendations\n",
    "for trainset, testset in kf.split(data):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    # iterate over all n_rec (number of recommendations): retrieve precision/recall for top k recommendations\n",
    "    for k in n_rec:\n",
    "        precisions, recalls = precision_recall_at_k(predictions, k, threshold)\n",
    "\n",
    "        # Precision and recall can then be averaged over all users\n",
    "        precisions_SlopeOne_dict[k].append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "        recalls_SlopeOne_dict[k].append(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "\n",
    "# send completion message via email (server, sender, recepient according to .env)\n",
    "ssm.sendstatus(\"SlopeOne precision/recall@k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing reproducibility of splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "kf = KFold(n_splits=5, random_state=42)\n",
    "for trainset, testset in kf.split(data):\n",
    "    print(trainset.global_mean)\n",
    "\n",
    "# result #1\n",
    "# 3.473827167636637\n",
    "# 3.4742678511810365\n",
    "# 3.4738866842197114\n",
    "# 3.4744854068183715\n",
    "# 3.4738730499427124\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "    print(trainset.global_mean)\n",
    "\n",
    "# result #2\n",
    "# 3.473827167636637\n",
    "# 3.4742678511810365\n",
    "# 3.4738866842197114\n",
    "# 3.4744854068183715\n",
    "# 3.4738730499427124\n",
    "\n",
    "# => reproducibility is given\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all results in one dict and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all dicts with precisions and recalls in one dict\n",
    "surp_precision_at_k_recall_at_k = {}\n",
    "surp_precision_at_k_recall_at_k['precisions_knnBasic_dict'] = precisions_knnBasic_dict\n",
    "surp_precision_at_k_recall_at_k['recalls_knnBasic_dict'] = recalls_knnBasic_dict\n",
    "surp_precision_at_k_recall_at_k['precisions_knnMean_dict'] = precisions_knnMean_dict\n",
    "surp_precision_at_k_recall_at_k['recalls_knnMean_dict'] = recalls_knnMean_dict\n",
    "surp_precision_at_k_recall_at_k['precisions_knnBaseline_dict'] = precisions_knnBaseline_dict\n",
    "surp_precision_at_k_recall_at_k['recalls_knnBaseline_dict'] = recalls_knnBaseline_dict\n",
    "surp_precision_at_k_recall_at_k['precisions_knnZScore_dict'] = precisions_knnZScore_dict\n",
    "surp_precision_at_k_recall_at_k['recalls_knnZScore_dict'] = recalls_knnZScore_dict\n",
    "surp_precision_at_k_recall_at_k['precisions_SVD_dict'] = precisions_SVD_dict\n",
    "surp_precision_at_k_recall_at_k['recalls_SVD_dict'] = recalls_SVD_dict\n",
    "surp_precision_at_k_recall_at_k['precisions_NMF_dict'] = precisions_NMF_dict\n",
    "surp_precision_at_k_recall_at_k['recalls_NMF_dict'] = recalls_NMF_dict\n",
    "surp_precision_at_k_recall_at_k['precisions_Baseline_dict'] = precisions_Baseline_dict\n",
    "surp_precision_at_k_recall_at_k['recalls_Baseline_dict'] = recalls_Baseline_dict\n",
    "surp_precision_at_k_recall_at_k['precisions_CC_dict'] = precisions_CC_dict\n",
    "surp_precision_at_k_recall_at_k['recalls_CC_dict'] = recalls_CC_dict\n",
    "surp_precision_at_k_recall_at_k['precisions_rand_dict'] = precisions_rand_dict\n",
    "surp_precision_at_k_recall_at_k['recalls_rand_dict'] = recalls_rand_dict\n",
    "surp_precision_at_k_recall_at_k['precisions_SlopeOne_dict'] = precisions_SlopeOne_dict\n",
    "surp_precision_at_k_recall_at_k['recalls_SlopeOne_dict'] = recalls_SlopeOne_dict\n",
    "\n",
    "# save dict to pkl\n",
    "joblib.dump(surp_precision_at_k_recall_at_k, '../models/surp_precision_at_k_recall_at_k.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization attempt (deactivatet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# sns.boxplot([precisions_knnZScore_dict[3],precisions_knnZScore_dict[5],\n",
    "#              precisions_knnZScore_dict[10],precisions_knnZScore_dict[20]]);\n",
    "# sns.relplot(data=precisions_knnZScore_dict,x=[1,2,3,4,5],y=precisions_knnZScore_dict[3], label='k=3')\n",
    "# sns.relplot(data=precisions_knnZScore_dict,x=[1,2,3,4,5],y=precisions_knnZScore_dict[5])\n",
    "# #sns.relplot(data=precisions_knnZScore_dict,x=[3,5,10,20],y=(np.mean(precisions_knnZScore_dict[3])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

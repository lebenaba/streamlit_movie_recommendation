{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import joblib\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from surprise.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go \n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some algorithms randomly initialize their parameters (sometimes with numpy), and the cross-validation folds are also randomly generated. \n",
    "If you need to reproduce your experiments multiple times, you just have to set the seed of the RNG at the beginning of your program:\"\"\"\n",
    "\n",
    "my_seed = 42\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"><b>Caution:</b> Always clear all output before pushing to GitHub! This reduces size form 70MB to under 1MB.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write preprocessed date into a DataFrame\n",
    "df = pd.read_csv('../data/processed/preprocessed_data_movielens.csv')\n",
    "df.drop(columns=['title','genres','relevance','tag'], inplace=True)\n",
    "df = df[['userId', 'movieId', 'rating']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into Surprise format, columns have been sorted in required order (raw user id, raw item id, rating) beforehand\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(df, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {\n",
    "    'name': 'cosine',\n",
    "    'user_based': False  # Compute similarities between items\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### knn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic\n",
    "\n",
    "algo = KNNBasic(sim_options=sim_options)\n",
    "\n",
    "# Train the model\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Generating Predictions\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculating performance metrics\n",
    "from surprise import accuracy\n",
    "\n",
    "mae_knnb = accuracy.mae(predictions, verbose=True)\n",
    "mse_knnb = accuracy.mse(predictions, verbose=True)\n",
    "rmse_knnb = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae_knnb:.4f}\")\n",
    "print(f\"Mean Square Error (MSE): {mse_knnb:.4f}\")\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse_knnb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNWithMeans\n",
    "\n",
    "algo = KNNWithMeans(sim_options=sim_options)\n",
    "\n",
    "# Train the model\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Generating Predictions\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculating performance metrics\n",
    "from surprise import accuracy\n",
    "\n",
    "mae_knnm = accuracy.mae(predictions, verbose=True)\n",
    "mse_knnm = accuracy.mse(predictions, verbose=True)\n",
    "rmse_knnm = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "# print(f\"Mean Absolute Error (MAE): {mae_knnm:.4f}\")\n",
    "# print(f\"Mean Square Error (MSE): {mse_knnm:.4f}\")\n",
    "# print(f\"Root Mean Square Error (RMSE): {rmse_knnm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBaseline\n",
    "\n",
    "algo = KNNBaseline(sim_options=sim_options)\n",
    "\n",
    "# Train the model\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Generating Predictions\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculating performance metrics\n",
    "from surprise import accuracy\n",
    "\n",
    "mae_knnbl = accuracy.mae(predictions, verbose=True)\n",
    "mse_knnbl = accuracy.mse(predictions, verbose=True)\n",
    "rmse_knnbl = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "# print(f\"Mean Absolute Error (MAE): {mae_knnbl:.4f}\")\n",
    "# print(f\"Mean Square Error (MSE): {mse_knnbl:.4f}\")\n",
    "# print(f\"Root Mean Square Error (RMSE): {rmse_knnbl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNWithZScore\n",
    "\n",
    "algo = KNNWithZScore(sim_options=sim_options)\n",
    "\n",
    "# Train the model\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Generating Predictions\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculating performance metrics\n",
    "from surprise import accuracy\n",
    "\n",
    "mae_knnz = accuracy.mae(predictions, verbose=True)\n",
    "mse_knnz = accuracy.mse(predictions, verbose=True)\n",
    "rmse_knnz = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "# print(f\"Mean Absolute Error (MAE): {mae_knnz:.4f}\")\n",
    "# print(f\"Mean Square Error (MSE): {mse_knnz:.4f}\")\n",
    "# print(f\"Root Mean Square Error (RMSE): {rmse_knnz:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matrix factorization models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "\n",
    "algo = SVD()\n",
    "\n",
    "# Train the model\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Generating Predictions\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculating performance metrics\n",
    "from surprise import accuracy\n",
    "\n",
    "mae_svd = accuracy.mae(predictions, verbose=True)\n",
    "mse_svd = accuracy.mse(predictions, verbose=True)\n",
    "rmse_svd = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "# print(f\"Mean Absolute Error (MAE): {mae_svd:.4f}\")\n",
    "# print(f\"Mean Square Error (MSE): {mse_svd:.4f}\")\n",
    "# print(f\"Root Mean Square Error (RMSE): {rmse_svd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NMF\n",
    "\n",
    "algo = NMF()\n",
    "\n",
    "# Train the model\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Generating Predictions\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculating performance metrics\n",
    "from surprise import accuracy\n",
    "\n",
    "mae_nmf = accuracy.mae(predictions, verbose=True)\n",
    "mse_nmf = accuracy.mse(predictions, verbose=True)\n",
    "rmse_nmf = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "# print(f\"Mean Absolute Error (MAE): {mae_nmf:.4f}\")\n",
    "# print(f\"Mean Square Error (MSE): {mse_nmf:.4f}\")\n",
    "# print(f\"Root Mean Square Error (RMSE): {rmse_nmf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NormalPredictor\n",
    "\n",
    "algo = NormalPredictor() # no sim_options possible\n",
    "\n",
    "# Train the model\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Generating Predictions\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculating performance metrics\n",
    "from surprise import accuracy\n",
    "\n",
    "mae_rand = accuracy.mae(predictions, verbose=True)\n",
    "mse_rand = accuracy.mse(predictions, verbose=True)\n",
    "rmse_rand = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "# print(f\"Mean Absolute Error (MAE): {mae_rand:.4f}\")\n",
    "# print(f\"Mean Square Error (MSE): {mse_rand:.4f}\")\n",
    "# print(f\"Root Mean Square Error (RMSE): {rmse_rand:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimations = []\n",
    "# for _, _, _, est, _ in predictions:\n",
    "#     estimations.append(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('mean real ratings:', np.mean(df.rating))\n",
    "# print('mean estimation:', np.mean(estimations))\n",
    "# plt.subplot(121)\n",
    "# plt.hist(df.rating, bins=10, rwidth=0.8)\n",
    "# plt.subplot(122)\n",
    "# plt.hist(estimations, bins=10, rwidth=0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import BaselineOnly\n",
    "\n",
    "algo = BaselineOnly()\n",
    "\n",
    "# Train the model\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Generating Predictions\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculating performance metrics\n",
    "from surprise import accuracy\n",
    "\n",
    "mae_base = accuracy.mae(predictions, verbose=True)\n",
    "mse_base = accuracy.mse(predictions, verbose=True)\n",
    "rmse_base = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "# print(f\"Mean Absolute Error (MAE): {mae_base:.4f}\")\n",
    "# print(f\"Mean Square Error (MSE): {mse_base:.4f}\")\n",
    "# print(f\"Root Mean Square Error (RMSE): {rmse_base:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SlopeOne\n",
    "\n",
    "algo = SlopeOne()\n",
    "\n",
    "# Train the model\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Generating Predictions\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculating performance metrics\n",
    "from surprise import accuracy\n",
    "\n",
    "mae_so = accuracy.mae(predictions, verbose=True)\n",
    "mse_so = accuracy.mse(predictions, verbose=True)\n",
    "rmse_so = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "# print(f\"Mean Absolute Error (MAE): {mae_so:.4f}\")\n",
    "# print(f\"Mean Square Error (MSE): {mse_so:.4f}\")\n",
    "# print(f\"Root Mean Square Error (RMSE): {rmse_so:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import CoClustering\n",
    "\n",
    "algo = CoClustering()\n",
    "\n",
    "# Train the model\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Generating Predictions\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Calculating performance metrics\n",
    "from surprise import accuracy\n",
    "\n",
    "mae_cc = accuracy.mae(predictions, verbose=True)\n",
    "mse_cc = accuracy.mse(predictions, verbose=True)\n",
    "rmse_cc = accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "# print(f\"Mean Absolute Error (MAE): {mae_cc:.4f}\")\n",
    "# print(f\"Mean Square Error (MSE): {mse_cc:.4f}\")\n",
    "# print(f\"Root Mean Square Error (RMSE): {rmse_cc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical interpretation of performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"><b>Caution:</b> Always clear all output before pushing to GitHub! This reduces size form 70MB to under 1MB.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_knnb = [mae_knnb, mse_knnb, rmse_knnb]\n",
    "metrics_knnm = [mae_knnm, mse_knnm, rmse_knnm]\n",
    "metrics_knnbl = [mae_knnbl, mse_knnbl, rmse_knnbl]\n",
    "metrics_knnz = [mae_knnz, mse_knnz, rmse_knnz]\n",
    "metrics_rand = [mae_rand, mse_rand, rmse_rand]\n",
    "metrics_so = [mae_so, mse_so, rmse_so]\n",
    "metrics_base = [mae_base, mse_base, rmse_base]\n",
    "metrics_svd = [mae_svd, mse_svd, rmse_svd]\n",
    "metrics_nmf = [mae_nmf, mse_nmf, rmse_nmf]\n",
    "metrics_cc = [mae_cc, mse_cc, rmse_cc]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x = ['MAE', 'MSE', 'RMSE'], y = metrics_rand, name = 'NormalPredictor', orientation='v')) \n",
    "fig.add_trace(go.Bar(x = ['MAE', 'MSE', 'RMSE'], y = metrics_knnb, name = 'KNNBasic', orientation='v'))\n",
    "fig.add_trace(go.Bar(x = ['MAE', 'MSE', 'RMSE'], y = metrics_knnm, name = 'KNNWithMeans', orientation='v')) #, marker_color = 'mediumturquoise'\n",
    "fig.add_trace(go.Bar(x = ['MAE', 'MSE', 'RMSE'], y = metrics_knnbl, name = 'KNNBaseline', orientation='v')) \n",
    "fig.add_trace(go.Bar(x = ['MAE', 'MSE', 'RMSE'], y = metrics_knnz, name = 'KNNWithZScore', orientation='v')) \n",
    "fig.add_trace(go.Bar(x = ['MAE', 'MSE', 'RMSE'], y = metrics_so, name = 'SlopeOne', orientation='v')) \n",
    "fig.add_trace(go.Bar(x = ['MAE', 'MSE', 'RMSE'], y = metrics_base, name = 'BaselineOnly', orientation='v')) \n",
    "fig.add_trace(go.Bar(x = ['MAE', 'MSE', 'RMSE'], y = metrics_svd, name = 'SVD', orientation='v')) \n",
    "fig.add_trace(go.Bar(x = ['MAE', 'MSE', 'RMSE'], y = metrics_nmf, name = 'NMF', orientation='v')) \n",
    "fig.add_trace(go.Bar(x = ['MAE', 'MSE', 'RMSE'], y = metrics_cc, name = 'CoClustering', orientation='v')) \n",
    "\n",
    "\n",
    "fig.update_layout(title = 'Performance metrics for different models of the Surprise library', title_x = 0.5, title_y=0.87, xaxis_title = 'Metric') # Title and axis titles\n",
    "fig.update_layout(autosize=False, width=1000, height=400) #,legend=dict(orientation=\"h\", y=-0.1)) # Figure size\n",
    "# layout = go.Layout(legend=dict(orientation=\"v\"))\n",
    "#fig.update_yaxes(range = [0.6,1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=df.rating))\n",
    "fig.update_layout(title = 'Rating distribution', title_x = 0.5, xaxis_title = 'rating', yaxis_title = 'frequency') # Title and axis titles\n",
    "fig.update_layout(autosize=False, width=600, height=400, ) # Figure size\n",
    "fig.update_layout(bargap=0.2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "surp_metrics_default_models = {}\n",
    "surp_metrics_default_models['knnBasic'] = metrics_knnb\n",
    "surp_metrics_default_models['knnMeans'] = metrics_knnm\n",
    "surp_metrics_default_models['knnBaseline'] = metrics_knnbl\n",
    "surp_metrics_default_models['knnZScore'] = metrics_knnz\n",
    "surp_metrics_default_models['SVD'] = metrics_svd\n",
    "surp_metrics_default_models['NMF'] = metrics_nmf\n",
    "surp_metrics_default_models['NormalPredictor'] = metrics_rand\n",
    "surp_metrics_default_models['SlopeOne'] = metrics_so\n",
    "surp_metrics_default_models['BaselineOnly'] = metrics_base\n",
    "surp_metrics_default_models['CoClustering'] = metrics_cc\n",
    "\n",
    "# save dict to pkl\n",
    "joblib.dump(surp_metrics_default_models, '../models/surp_metrics_default_models.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyparsing import col\n",
    "\n",
    "\n",
    "metrics = joblib.load('../models/surp_metrics_default_models.pkl')\n",
    "\n",
    "# create empty DataFrame with columns according to metrics\n",
    "keys = list(metrics.keys()) # list of keys, which hold the model names\n",
    "df_metrics = pd.DataFrame(metrics, index=['MAE','MSE','RMSE']).T # e.g. use first model to retrieve coumns\n",
    "\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"><b>Caution:</b> Always clear all output before pushing to GitHub! This reduces size form 70MB to under 1MB.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with whole dataset? (inactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# from surprise import BaselineOnly, Dataset, Reader\n",
    "# from surprise.model_selection import cross_validate\n",
    "\n",
    "# # path to dataset file\n",
    "# file_path = '../data/raw/ml-25m/ratings.csv'\n",
    "\n",
    "# # As we're loading a custom dataset, we need to define a reader. In the movielens-25M dataset, each line has the following format:\n",
    "# # 'user item rating timestamp', separated by ',' characters.\n",
    "# # the rating scale differs from the default which is (1,5)\n",
    "# # the first line of the csv-file holds the column labels and has to be skipped\n",
    "# reader = Reader(line_format=\"user item rating timestamp\", sep=\",\", rating_scale=(0.5,5.0), skip_lines=1)\n",
    "\n",
    "# data_raw = Dataset.load_from_file(file_path, reader=reader)\n",
    "\n",
    "# # We can now use this dataset as we please, e.g. calling cross_validate\n",
    "# cross_validate(BaselineOnly(), data_raw, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define sim_options to be tested\n",
    "# sim_options = {\n",
    "# \"name\": [\"msd\", \"cosine\", \"pearson\"],\n",
    "# \"min_support\": [3, 4, 5],\n",
    "# \"user_based\": [False], # only item-base approach, since it is generally better suited for the task and user based would require enormous amounts of memory\n",
    "# }\n",
    "# param_grid = {\"sim_options\": sim_options,\n",
    "#               \"k\": [20, 30, 40], # The (max) number of neighbors to take into account for aggregation\n",
    "#               \"min_k\": [1, 2, 3]} # The minimum number of neighbors to take into account for aggregation\n",
    "# gs = GridSearchCV(KNNWithMeans, param_grid, measures=[\"rmse\", \"mse\", \"mae\"], cv=3)\n",
    "# gs.fit(data)\n",
    "# print(gs.best_score[\"rmse\"])\n",
    "# print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gs.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save GridSearchCV object to file in models folder\n",
    "# import joblib\n",
    "# joblib.dump(gs, '../models/surp_gridsearchcv_knnMeans.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try if it can be loaded successfully\n",
    "# import joblib\n",
    "# gs_loaded = joblib.load('../models/surp_gridsearchcv_knnMeans.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gs_loaded.best_score)\n",
    "# print(gs_loaded.best_params['rmse']['sim_options'])\n",
    "# print(gs_loaded.best_params['rmse']['k'])\n",
    "# print(gs_loaded.best_params['rmse']['min_k'])\n",
    "# gs_loaded.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo = KNNWithMeans(sim_options=gs_loaded.best_params['rmse']['sim_options'],\n",
    "#                     k=gs_loaded.best_params['rmse']['k'],\n",
    "#                     min_k=gs_loaded.best_params['rmse']['min_k'])\n",
    "\n",
    "# # Train the model\n",
    "# algo.fit(trainset)\n",
    "\n",
    "# # Generating Predictions\n",
    "# predictions = algo.test(testset)\n",
    "\n",
    "# # Calculating performance metrics\n",
    "# from surprise import accuracy\n",
    "\n",
    "# mae_knnm = accuracy.mae(predictions, verbose=True)\n",
    "# mse_knnm = accuracy.mse(predictions, verbose=True)\n",
    "# rmse_knnm = accuracy.rmse(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "# from surprise import KNNWithMeans\n",
    "# from surprise.model_selection import cross_validate\n",
    "\n",
    "# my_seed = 42\n",
    "# random.seed(my_seed)\n",
    "# np.random.seed(my_seed)\n",
    "\n",
    "# measure = 'mae'\n",
    "# # We can now use this dataset as we please, e.g. calling cross_validate\n",
    "# cv_knnMeans = cross_validate(KNNWithMeans(sim_options=gs_loaded.best_params[measure]['sim_options'],\n",
    "#                                       k=gs_loaded.best_params[measure]['k'],\n",
    "#                                       min_k=gs_loaded.best_params[measure]['min_k']),\n",
    "#                         data, measures=[\"MAE\", \"MSE\", \"RMSE\"], n_jobs=-1, verbose=True)\n",
    "\n",
    "# print(cv_knnMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save cross-validation object to file in models folder\n",
    "# joblib.dump(cv_knnm, '../models/surp_cv_GS_winner_knnMeans.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_loaded = joblib.load('../models/surp_cv_GS_winner_knnMeans.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can now use this dataset as we please, e.g. calling cross_validate\n",
    "# sim_options = {\n",
    "#     'name': 'cosine',\n",
    "#     'user_based': False  # Compute similarities between items\n",
    "# }\n",
    "# cv_knnb = cross_validate(KNNBasic(sim_options=sim_options), data, verbose=True)\n",
    "# print(cv_knnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Element 0 of dict:', cv_knnb['test_rmse'][0])\n",
    "# print('RSME mean:', cv_knnb['test_rmse'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We can now use this dataset as we please, e.g. calling cross_validate\n",
    "# cross_validate(BaselineOnly(), data, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a whole trainset and the predict() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from surprise import KNNBasic\n",
    "\n",
    "# # Retrieve the trainset.\n",
    "# trainset_full = data.build_full_trainset()\n",
    "\n",
    "# # Build an algorithm, and train it.\n",
    "# sim_options_full = {\n",
    "#     'name': 'cosine',\n",
    "#     'user_based': False  # Compute similarities between items\n",
    "# }\n",
    "# algo_full = KNNBasic(sim_options=sim_options_full)\n",
    "# algo_full.fit(trainset_full)\n",
    "\n",
    "# uid = str(74244)  # raw user id (as in the ratings file). They are **strings**!\n",
    "# iid = str(1)  # raw item id (as in the ratings file). They are **strings**!\n",
    "\n",
    "# # get a prediction for specific users and items. r_ui represents the true, known rating.\n",
    "# pred = algo_full.predict(uid, iid, r_ui=4, verbose=True)\n",
    "\n",
    "# # Note: The predict() uses raw ids. As the dataset we have used has been read from a file, the raw ids are strings (even if they represent numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uid = str(124114)  # raw user id (as in the ratings file). They are **strings**!\n",
    "# iid = str(542)  # raw item id (as in the ratings file). They are **strings**!\n",
    "\n",
    "# # get a prediction for specific users and items. r_ui represents the true, known rating.\n",
    "# pred = algo_full.predict(uid, iid, r_ui=4, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
